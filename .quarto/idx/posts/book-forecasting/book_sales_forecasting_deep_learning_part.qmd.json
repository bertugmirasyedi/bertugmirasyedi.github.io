{"title":"Book Sales Forecasting: Comparison of Different Models Part 2","markdown":{"yaml":{"title":"Book Sales Forecasting: Comparison of Different Models Part 2","description":"Deep Learning Models","date":"2022-12-20","image":"images/forecast.png","categories":["python","time series forecasting","tensorflow","optuna","hyperparameter tuning"],"execute":{"freeze":true},"jupyter":"tensorflow-test","highlight-style":"breezedark","toc":true,"number-sections":true},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis post is the continuation of the [book sales forecasting case](/posts/book-forecasting/book_sales_forecasting.html). In this second part we will create deep learning models and hyperparameter tuning according to the assumed business case on Part 1.\n    \n# Deep Learning\n\nNow we will create the forecasts with deep learning methods - LSTM and CNN. For these models we have to change feature and target DataFrames to Numpy arrays, and reshape features to 3 dimensional shape of (samples, timesteps, features). After creating a base model for both of them and comparing visually by plotting test features with forecasts, we will create our custom function in the next section and do hyperparameter tuning.\n\n## LSTM\n\n```{python}\n#| eval: true\n#| echo: false\nimport matplotlib.pyplot as plt\nimport pandas\nimport seaborn as sns\nimport numpy as np\nfrom statsmodels.tsa.forecasting.stl import STLForecast\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# Setting plot style and size\nsns.set_style(\"darkgrid\")\nplt.rc(\"figure\", figsize=(10, 6))\nplt.rc(\"font\", size=10)\n\n\nsubset = pandas.read_csv(\"subset_sklearn.csv\")\nsubset = subset.drop(\n  columns=[\n    \"Unnamed: 0\",\n    \"0\",\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\"]\n)\n\ntrain_subset = subset[0:1095]\ntest_subset = subset[1095:1461]\n\n# Define train/test features and targets\ntrain_targets = train_subset[\"num_sold\"]\ntrain_features = train_subset.drop(columns=\"num_sold\")\ntest_targets = test_subset[\"num_sold\"]\ntest_features = test_subset.drop(columns=\"num_sold\")\n\n# Create a function to plot both test data and forecast from the model to compare them visually.\ndef plot_forecasts(y_true, y_pred, title=\"Forecasts\", width=700, height=500):\n    import plotly.express as px\n\n    # Create a DataFrame to store both series\n    series = pandas.DataFrame()\n    series[\"Observed\"] = y_true\n    series[\"Predicted\"] = y_pred\n    \n    # Define plot attributes\n    fig = px.line(\n        series, \n        width=width, \n        height=height, \n        title=title,\n        labels={\n            \"value\": \"Sales\",\n            \"variable\": \"\"\n        }\n    )\n    fig.show()\n```\n\nFirstly, we will create a simple network.\n\n```{python}\n#| code-fold: false\n#| code-summary: \"Simple LSTM Network\"\n#| echo: true\n#| eval: false\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Sequential\n\n# Disable logging\ntf.keras.utils.disable_interactive_logging()\n\nwindow_size = 7\n\n# Create train and test data for LSTM\nlstm_train_features = np.array(train_features)\nlstm_train_targets = np.array(train_targets)\n\nlstm_test_features = np.array(test_features)\nlstm_test_targets = np.array(test_targets)\n\n# Reshape train and test features suitable fo RNN\nlstm_train_features = lstm_train_features.reshape((lstm_train_features.shape[0], 1, lstm_train_features.shape[1]))\n\nlstm_test_features = lstm_test_features.reshape((lstm_test_features.shape[0], 1, lstm_test_features.shape[1]))\n\n# Implement LSTM\nlstm_model = Sequential()\nlstm_model.add(layers.LSTM(50, activation=\"relu\"))\nlstm_model.add(layers.Dense(1))\nlstm_model.compile(loss=\"mape\", optimizer=\"Adam\")\n\n# Fit and Forecast\nlstm_model.fit(lstm_train_features, lstm_train_targets, 1, 5, verbose=0)\nlstm_forecast = lstm_model.predict(lstm_test_features)\n```\n\n```{python}\n#| echo: false\n#| eval: true\n#| warning: false\n#| cache: true\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Sequential\n\n# Disable logging\ntf.keras.utils.disable_interactive_logging()\n\nwindow_size = 7\n\n# Create train and test data for LSTM\nlstm_train_features = np.array(train_features)\nlstm_train_targets = np.array(train_targets)\n\nlstm_test_features = np.array(test_features)\nlstm_test_targets = np.array(test_targets)\n\n# Reshape train and test features suitable fo RNN\nlstm_train_features = lstm_train_features.reshape((lstm_train_features.shape[0], 1, lstm_train_features.shape[1]))\n\nlstm_test_features = lstm_test_features.reshape((lstm_test_features.shape[0], 1, lstm_test_features.shape[1]))\n\n# Implement LSTM\nlstm_model = Sequential()\nlstm_model.add(layers.LSTM(50, activation=\"relu\"))\nlstm_model.add(layers.Dense(1))\nlstm_model.compile(loss=\"mape\", optimizer=\"Adam\")\n\n# Fit and Forecast\nlstm_model.fit(lstm_train_features, lstm_train_targets, 1, 5, verbose=0);\nlstm_forecast = lstm_model.predict(lstm_test_features)\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(lstm_test_targets, lstm_forecast, title=\"Simple LSTM Model Forecasts\");\n```\n\n```{python}\n#| echo: false\ndef mean_absolute_percentage_error(y_true, y_pred):\n  import numpy as np\n\n  ape = [abs(y_true[i] - y_pred[i])/y_true[i] for i in range(len(y_true))]\n  mape = np.mean(ape)\n  return mape\n\ndef r2_score(y_true, y_pred):\n  import numpy as np\n  import math\n    \n  mean = np.mean(y_true)\n  ss_res = 0\n  ss_tot = 0\n  \n  for i in range(len(y_true)):\n    ss_res += math.pow((y_true[i] - y_pred[i]), 2)\n    ss_tot += math.pow((y_true[i] - mean), 2)\n    \n  score = 1 - (ss_res/ss_tot)\n  return score\n```\n\nFor easier comparison we will compute the previous benchmarks again.\n\n```{python}\n#| code-fold: true\n#| code-summary: \"LSTM Benchmark Scores\"\nprint(f\"MAPE for LSTM model is: {mean_absolute_percentage_error(lstm_test_targets, lstm_forecast)}\")\nprint(f\"R2 Score for LSTM model is: {r2_score(lstm_test_targets, lstm_forecast)}\")\n```\n\nNow let's create a simple CNN network and plot its forecasts.\n\n## CNN\n\n```{python}\n#| code-fold: true\n#| code-summary: \"Simple CNN Model\"\n#| cache: true\n#| warning: false\n\n# Disable logging\ntf.keras.utils.disable_interactive_logging()\n\n# Create train and test data for CNN\ncnn_train_features = np.array(train_features)\ncnn_train_targets = np.array(train_targets)\n\ncnn_test_features = np.array(test_features)\ncnn_test_targets = np.array(test_targets)\n\n# Reshape train and test features suitable fo RNN\ncnn_train_features = cnn_train_features.reshape((cnn_train_features.shape[0], 1, cnn_train_features.shape[1]))\ncnn_test_features = cnn_test_features.reshape((cnn_test_features.shape[0], 1, cnn_test_features.shape[1]))\n\n# Implement CNN\ncnn_model = Sequential()\ncnn_model.add(layers.Conv1D(50, 1, activation=\"relu\"))\ncnn_model.add(layers.Flatten())\ncnn_model.add(layers.Dense(1))\ncnn_model.compile(loss=\"mape\", optimizer=\"Adam\")\n\n# Fit and Forecast\ncnn_model.fit(cnn_train_features, cnn_train_targets, 1, 5, verbose=0);\ncnn_forecast = cnn_model.predict(cnn_test_features)\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(cnn_test_targets, cnn_forecast, title=\"Simple CNN Model Forecasts\");\n```\n\nPlots show comparable performances for simple LSTM and CNN models. Let's quantify the comparison.\n\n```{python}\n#| echo: false\nprint(f\"MAPE for CNN model is: {mean_absolute_percentage_error(cnn_test_targets, cnn_forecast)}\")\nprint(f\"R2 Score for CNN model is: {r2_score(cnn_test_targets, cnn_forecast)}\")\n```\n\n# RNN Model Tuning\n\nIn this deep learning tuning part, we will combine tuning of LSTM and CNN models and also several other network types under one category of **RNN Model**. The code will compare pure LSTM, pure CNN, Stacked LSTM, Bidirectional LSTM and CNN-LSTM Models together and select the best performing one.\n\n```{python}\n#| echo: false\n# Define cost function for our case\ndef cost_function(storage, y_true, y_pred, book_price, storage_monthly_rent):\n  if storage == 0:\n    if y_pred <= y_true:\n      cost = book_price*(y_true - y_pred)\n      to_storage = 0\n      return cost, to_storage\n    elif y_pred > y_true:\n      cost = storage_monthly_rent/30\n      to_storage = y_pred - y_true\n      return cost, to_storage\n  elif storage > 0:\n    if y_pred <= y_true:\n      if (y_true - y_pred) < storage:\n        cost = storage_monthly_rent/30\n        to_storage = y_pred - y_true\n        return cost, to_storage\n      elif (y_true - y_pred) > storage:\n        cost = book_price*(y_true - y_pred - storage) + storage_monthly_rent/30\n        to_storage = -storage\n        return cost, to_storage\n    elif y_pred > y_true:\n      cost = storage_monthly_rent/30\n      to_storage = y_pred - y_true\n      return cost, to_storage\n```\n\n```{python}\n#| cache: true\n#| code-fold: true\n#| code-summary: \"RNN Model Tuning Code\"\n#| warning: false\nimport optuna\nimport warnings\nfrom tensorflow.keras.layers import Bidirectional\nwarnings.filterwarnings(\"ignore\")\n\n# Disable logging\ntf.keras.utils.disable_interactive_logging()\n\n# Define model creation function\ndef create_rnn_model(trial):\n  # Define trial variables\n  model_type = trial.suggest_categorical(\n      \"model_type\",\n      [\"vanilla_lstm\", \"stacked_lstm\", \"bidirectional_lstm\", \"cnn\", \"cnn_lstm\"],\n  )\n  \n  dropout = trial.suggest_categorical(\"dropout\", [True, False])\n  \n  if model_type == \"vanilla_lstm\":\n    # Trial variables\n    units = trial.suggest_int(\"units\", 50, 200)\n    dense_layers = trial.suggest_int(\"dense_layers\", 0, 2)\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n    \n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(layers.LSTM(units, activation=activation))\n    \n    for layer in range(dense_layers):\n      rnn_model.add(layers.Dense(units, activation=\"relu\"))\n        \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n    \n    \n  elif model_type == \"stacked_lstm\":\n    # Trial variables\n    units = trial.suggest_int(\"units\", 50, 200)\n    dense_layers = trial.suggest_int(\"dense_layers\", 0, 2)\n    lstm_layers = trial.suggest_int(\"lstm_layers\", 2, 3)\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n    \n    # Define model with trial variables\n    rnn_model = Sequential()\n    \n    for layer in range(lstm_layers):\n      if layer == lstm_layers - 1:\n        rnn_model.add(layers.LSTM(units, activation=activation))\n      else:\n        rnn_model.add(layers.LSTM(units, activation=activation, return_sequences=True))\n    \n    for layer in range(dense_layers):\n      rnn_model.add(layers.Dense(units, activation=\"relu\"))\n    \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n    \n  elif model_type == \"bidirectional_lstm\":\n    # Trial variables\n    units = trial.suggest_int(\"units\", 50, 200)\n    dense_layers = trial.suggest_int(\"dense_layers\", 0, 2)\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n    \n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(Bidirectional(layers.LSTM(units, activation=activation)))\n    \n    for layer in range(dense_layers):\n      rnn_model.add(layers.Dense(units, activation=\"relu\"))\n    \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n  \n  elif model_type == \"cnn\":\n    # Trial variables\n    units = trial.suggest_int(\"units\", 50, 200)\n    dense_layers = trial.suggest_int(\"dense_layers\", 0, 2)\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n    \n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(layers.Conv1D(units, 1, activation=activation))\n    \n    for layer in range(dense_layers):\n      rnn_model.add(layers.Dense(units, activation=\"relu\"))\n      \n    rnn_model.add(layers.Flatten())\n    \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n    \n  elif model_type == \"cnn_lstm\":\n    # Trial variables\n    units = trial.suggest_int(\"units\", 50, 200)\n    dense_layers = trial.suggest_int(\"dense_layers\", 0, 2)\n    activation = trial.suggest_categorical(\"activation\", [\"relu\", \"tanh\"])\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n    \n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(layers.Conv1D(units, 1, activation=activation))\n    \n    for layer in range(dense_layers):\n      rnn_model.add(layers.Dense(units, activation=\"relu\"))\n    \n    rnn_model.add(layers.LSTM(units, activation=activation))\n    \n    for layer in range(dense_layers):\n      rnn_model.add(layers.Dense(units, activation=\"relu\"))\n    \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n    \n  return rnn_model\n\n\n# Define Optuna Objective\ndef rnn_objective(trial):\n  import tensorflow as tf\n  \n  # Define training variables\n  batch_size = trial.suggest_int(\"batch_size\", 1, 100, log=True)\n  epochs = trial.suggest_int(\"epochs\", 10, 100)\n  \n  # Call model\n  rnn_model = create_rnn_model(trial)\n  \n  # Fit the model\n  rnn_model.fit(lstm_train_features, lstm_train_targets, batch_size, epochs, verbose=0);\n\n  # Forecast for the test data\n  rnn_forecast = rnn_model.predict(lstm_test_features)\n\n  # Create a loop to calculate cumulative cost of the forecast\n  storage = 0\n  cumulative_cost = 0\n  book_price = 20\n  monthly_storage_cost = 100\n  for step in range(len(lstm_test_targets)):\n\n    # Get the cost and difference for storage for the current step\n    cost, to_storage = cost_function(\n      storage, \n      lstm_test_targets[step], \n      rnn_forecast[step][0], \n      book_price, \n      monthly_storage_cost)\n\n    # Add cost to cumulative cost and storage difference to storage\n    cumulative_cost += int(cost)\n    storage += int(to_storage)\n\n  total_cost = cumulative_cost + storage*book_price\n  return total_cost\n\n# Create Optuna Study and Minimize total_cost\nrnn_study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.QMCSampler())\nrnn_study.optimize(rnn_objective, n_trials=20)\n```\n\n```{python}\n#| code-fold: true\n#| cache: true\n#| code-summary: \"RNN Model with Best Parameters\"\n#| warning: false\n\n# Disable logging\ntf.keras.utils.disable_interactive_logging()\n\n# Define best model parameters\nbest_rnn_model_parameters = rnn_study.best_params\nmodel_type = best_rnn_model_parameters[\"model_type\"]\nunits = best_rnn_model_parameters[\"units\"]\nactivation = best_rnn_model_parameters[\"activation\"]\ndense_layers = best_rnn_model_parameters[\"dense_layers\"]\nlstm_layers = best_rnn_model_parameters[\"lstm_layers\"] if \"lstm_layers\" in best_rnn_model_parameters.keys() else 0\noptimizer = best_rnn_model_parameters[\"optimizer\"]\nbatch_size = best_rnn_model_parameters[\"batch_size\"]\nepochs = best_rnn_model_parameters[\"epochs\"]\ndropout = best_rnn_model_parameters[\"dropout\"]\n\ntest_subset = subset[730:1095]\nval_subset = subset[1095:1461]\n\ntest_targets = np.array(test_subset[\"num_sold\"])\ntest_features = np.array(test_subset.drop(columns=[\"num_sold\"])).reshape((test_subset.shape[0], 1, test_subset.shape[1] - 1))\nval_targets = np.array(val_subset[\"num_sold\"])\nval_features = np.array(val_subset.drop(columns=[\"num_sold\"])).reshape((val_subset.shape[0], 1, val_subset.shape[1] - 1))\n\n\nif model_type == \"vanilla_lstm\":\n\n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(layers.LSTM(units, activation=activation))\n\n    for layer in range(dense_layers):\n        rnn_model.add(layers.Dense(units, activation=\"relu\"))\n        \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n\n\nelif model_type == \"stacked_lstm\":\n\n    # Define model with trial variables\n    rnn_model = Sequential()\n\n    for layer in range(lstm_layers):\n        if layer == lstm_layers - 1:\n            rnn_model.add(layers.LSTM(units, activation=activation))\n        else:\n            rnn_model.add(layers.LSTM(units, activation=activation, return_sequences=True))\n\n    for layer in range(dense_layers):\n        rnn_model.add(layers.Dense(units, activation=\"relu\"))\n\n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n\n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n\nelif model_type == \"bidirectional_lstm\":\n\n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(Bidirectional(layers.LSTM(units, activation=activation)))\n\n    for layer in range(dense_layers):\n        rnn_model.add(layers.Dense(units, activation=\"relu\"))\n\n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n\n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n\nelif model_type == \"cnn\":\n\n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(layers.Conv1D(units, 1, activation=activation))\n\n    for layer in range(dense_layers):\n        rnn_model.add(layers.Dense(units, activation=\"relu\"))\n        \n    rnn_model.add(layers.Flatten())\n    \n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n    \n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n\nelif model_type == \"cnn_lstm\":\n\n    # Define model with trial variables\n    rnn_model = Sequential()\n    rnn_model.add(layers.Conv1D(units, 1, activation=activation))\n\n    for layer in range(dense_layers):\n        rnn_model.add(layers.Dense(units, activation=\"relu\"))\n\n    rnn_model.add(layers.LSTM(units, activation=activation))\n\n    for layer in range(dense_layers):\n        rnn_model.add(layers.Dense(units, activation=\"relu\"))\n\n    if dropout:\n      rnn_model.add(layers.Dropout(rate=0.25))\n\n    rnn_model.add(layers.Dense(1))\n    rnn_model.compile(loss=\"mae\", optimizer=optimizer)\n    \n# Fit the model\nrnn_model.fit(test_features,\n              test_targets,\n              batch_size,\n              epochs,\n              verbose=1,\n              validation_data=(val_features, val_targets),\n              callbacks=tf.keras.callbacks.EarlyStopping(patience=7, min_delta=0.1));\n\n# Forecast for the test data\nrnn_forecast = rnn_model.predict(val_features)\n\n# Calculate total cost\nstorage = 0\ncumulative_cost = 0\nbook_price = 20\nmonthly_storage_cost = 100\nfor step in range(len(val_targets)):\n\n    # Get the cost and difference for storage for the current step\n    cost, to_storage = cost_function(\n      storage, \n      val_targets[step], \n      rnn_forecast[step][0], \n      book_price, \n      monthly_storage_cost)\n\n    # Add cost to cumulative cost and storage difference to storage\n    cumulative_cost += int(cost)\n    storage += int(to_storage)\n\nrnn_model_total_cost = cumulative_cost + storage*book_price\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(val_targets, rnn_forecast, title=\"Simple LSTM Model Forecasts\");\n```\n\n```{python}\n#| echo: false\nprint(f\"Model Type of the Tuned RNN Model: {best_rnn_model_parameters['model_type']}\\n\")\n\nprint(f\"MAPE for Tuned RNN Model is: {mean_absolute_percentage_error(val_targets, rnn_forecast)}\")\nprint(f\"R2 Score for Tuned RNN Model is: {r2_score(val_targets, rnn_forecast)}\\n\")\n\nprint(f\"Total Cost for Tuned RNN Model is: €{rnn_model_total_cost:,.2f}\")\n```\n\n# Results and Conclusion\n\n```{python}\n#| echo: false\nimport os\ntuning_results = {\n  \"STL ARIMA\": 65375,\n  \"ARIMAX\": 618358,\n  \"XGBoost\": 61498,\n  \"RNN Model\": rnn_model_total_cost\n}\n\nos.environ[\"rnn_model_cost_str\"] = \"€{rnn_model_total_cost:,.2f}\"\n\ntuning_results = sorted(tuning_results.items(), key=lambda x:x[1])\nbest_model_name = tuning_results[0][0]\nbest_model_cost = tuning_results[0][1]\n```\n\nWe have tuned all models and calculated the total costs in terms of our cost function and the winner is **XGBoost**! XGBoost's performance is not surprising given that it is usually the winning model for competitions but STL-ARIMA has also performed very successfully. RNN models were, for me, somewhat disappointing; however, it should be stated that we didn't fully utilized LSTM cells' memory because we have only used one timestep in the input. Increasing timesteps could potentially improve its performance. ARIMAX was the worst performer even with tuning. It could be because of the feature matrix that we have created couldn't fully capture seasonality and trend.\n\nEven though we have added calendar features, they might not be enough to predict seasonality and trend. Therefore, a decomposition model where a model similar to STL decompose the time series to trend, seasonality, and residuals and after that a seperate regression model trained on only residuals could potentially be more accurate; but that's all for this post.\n\n**Thank you for reading this post and see you on the next one!**\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"highlight-style":"breezedark","toc":true,"number-sections":true,"output-file":"book_sales_forecasting_deep_learning_part.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","theme":"flatly","title-block-banner":true,"title":"Book Sales Forecasting: Comparison of Different Models Part 2","description":"Deep Learning Models","date":"2022-12-20","image":"images/forecast.png","categories":["python","time series forecasting","tensorflow","optuna","hyperparameter tuning"],"jupyter":"tensorflow-test"},"extensions":{"book":{"multiFile":true}}}}}