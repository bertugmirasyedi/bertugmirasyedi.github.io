{"title":"Book Sales Forecasting: Comparison of Different Models Part 1","markdown":{"yaml":{"title":"Book Sales Forecasting: Comparison of Different Models Part 1","description":"From Classical Statistical Models to XGBoost","date":"2022-12-19","image":"images/forecast.png","categories":["python","time series forecasting","xgboost","arima","optuna","hyperparameter tuning"],"execute":{"freeze":true},"jupyter":"python3","highlight-style":"breezedark","toc":true,"number-sections":true},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n![](images/forecast.png)\n\n\nTime series forecasting is one of the most important fields in statistics because it is arguably the most impactful for businesses. Every business would love to know what will happen tomorrow, next week, next year, and next 10 years and make decisions accordingly. Although statistical methods for forecasting is not quite the silver bullet, it can help us make reasonable predictions under certain assumptions. \n\nIn this post, we will look into several forecasting models - both classical methods (e.g. ARIMA) and deep learing (LSTM, CNN) - and compare their performance using hyperparameter optimization with Optuna. I will also assume a business case in the end and the best model will be the less costly one. Let's start with some data analysis and visualizations.\n\n# Data Analysis and Preprocessing\n\nFirstly we will import the training dataset `train.csv` using `pandas` and check its columns and some sample values.\n\n```{python}\n#| eval: true\n#| echo: false\n# Setting plot style and size\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\nplt.rc(\"figure\", figsize=(10, 6))\nplt.rc(\"font\", size=10)\n```\n\n```{python}\n#| warning: false\nimport pandas\n\nlocal_filepath = \"train.csv\"\ndata = pandas.read_csv(local_filepath)\ndata.sample()\n```\n\n```{python}\ndata.describe()\n```\n\n```{python}\n# Check for missing values\nTrue in data.isnull()\n```\n\nGood. The data don't have any missing values. Now let's check for what kind of values that the data have on each column.\n\n```{python}\nprint(f\"Unique country values are: {data['country'].unique()}\\n\")\nprint(f\"Unique store values are: {data['store'].unique()}\\n\")\nprint(f\"Unique product values are: {data['product'].unique()}\\n\")\nprint(f\"Unique dates are: {data['date'].unique()}\\n\")\n```\n\nSo the table has the daily sales data of **4 different books**, sold in **2 different stores**, which have branches in **6 different countries**, spanning from **start of 2017 to the end of 2020**. Data has 70128 rows which means for each book, store and country combination we have 4 years of sales data.\n\nLet's plot a subset of the whole data.\n\n```{python}\n#| fig-align: center\n#| code-fold: true\n#| warning: false\nimport plotly.express as px\n\nsubsets = pandas.DataFrame()\nsubsets[\"subset1\"] = data[\n    (data[\"country\"] == \"Germany\")\n    & (data[\"store\"] == \"KaggleMart\")\n    & (data[\"product\"] == \"Kaggle Advanced Techniques\")\n].set_index(\"date\")[\"num_sold\"]\n\nsubsets[\"subset2\"] = data[\n    (data[\"country\"] == \"Belgium\")\n    & (data[\"store\"] == \"KaggleRama\")\n    & (data[\"product\"] == \"Kaggle Getting Started\")\n].set_index(\"date\")[\"num_sold\"]\n\nsubsets[\"subset3\"] = data[\n    (data[\"country\"] == \"France\")\n    & (data[\"store\"] == \"KaggleMart\")\n    & (data[\"product\"] == \"Kaggle Recipe Book\")\n].set_index(\"date\")[\"num_sold\"]\n\nfig = px.line(\n    subsets,\n    width=700,\n    height=500,\n    title=\"Sales Data For Different Subsets\",\n    labels={\"value\": \"Sales\", \"date\": \"Date\", \"variable\": \"\"},\n)\nfig.show();\n```\n\nLooking at the plot, especially in the `Subset1`, there is a yearly seasonality in the sales. Also there is a sharp rise in the sales around New Year's Eve.\n\nBefore modelling, we have to check whether any extra data transformation is needed. Since the data has 48 subsets like the 3 subsets above, it has to be arranged such that a subset's all values for 4 years are in a sequence before continuing to another subset. Let's create a sample DataFrame to illustrate this point.\n\n```{python}\nimport numpy as np\n\nsample_countries = [\"Germany\", \"France\"]\nsample_stores = [\"KaggleRama\", \"KaggleMart\"]\nsample_timepoints = [0, 1, 2, 3]\nsample_num_sold = np.tile([10, 20, 30, 40], 4)\n\nidx = pandas.MultiIndex.from_product(\n    [sample_countries, sample_stores, sample_timepoints]\n)\nexample = pandas.DataFrame(index=idx, data={\"num_sold\": sample_num_sold})\nexample\n```\n\nNow transform the whole data.\n\n```{python}\nfrom pandas import to_datetime\n\n# First sort the data and drop the row_id column since we don't need it.\ntransformed_data = data.sort_values(by=[\"country\", \"store\", \"product\", \"date\"]).drop(\n    columns=\"row_id\"\n)\n\n# Change the date column to pandas Datetime\ntransformed_data[\"date\"] = to_datetime(transformed_data[\"date\"])\ntransformed_data = transformed_data.set_index([\"country\", \"store\", \"product\", \"date\"])\ntransformed_data\n```\n\n# Modelling\n\nThe data is ready to model. We will create several different models for forecasting: ARIMA, Regression with ARIMAX and XGBoost, and Deep Learning with Long Short Term Memory (LSTM). Each model may need another data transformation but it will be easy to transform from the data that we have already preprocessed.\n\nWe will not tune the hyperparameters rigorously at first, just enough to get reasonable forecasts. After we went through all of them, I will create a custom performance metric for an assumed business case and then we will compare each model according to this metric.\n\n## STL-ARIMA\n\nAutoregressive Integrated Moving Average (ARIMA) is a statistical method used for forecasting. It uses values at previous time points to predict current value (autoregression) and these regressions' errors (moving average). Main assumption of ARIMA is that the time series should be stationary meaning its value is independent of time. Trend and seasonality makes the time series non-stationary. To make time series stationary we can use \"differencing\" - simply subtracting the value at t-1 from the value at t. \"Integrated\" part of the ARIMA is referring to the differencing order to make time series stationary.\n\nARIMA consists of three parts each having their own order - Autoregression p, Integration d, and Moving Average q. We will use Autocorrelation Function and Partial Autocorrelation Function graphs to select correct orders.\n\nMoving to our data, just by looking the above plots we can say there is a strong seasonality in the time series but to quantify it we will use Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test to check for stationarity.\n\n```{python}\n#| warning: false\nfrom statsmodels.tsa.stattools import kpss\n\nkpss_test = kpss(transformed_data)\nprint(f\"p-value of KPSS test is: {kpss_test[1]}\")\n```\n\nNull hypothesis of KPSS test is that the series is stationary. Since the p-value of our test result is smaller that 0.05 we can reject the null hypothesis and state that the series is non-stationary.\n\nBefore creating the ARIMA model we have to remove trend and seasonality from the data. We could use the seasonal variant of ARIMA called **SARIMA**; however, our data's frequency is a day so it has a periodicity of 365 days. SARIMA models are more suitable for monthly or quarterly data; therefore we will use **Seasonal-Trend Decomposition Using LOESS (STL)** method to remove trend and seasonality from the data.\n\n```{python}\n#| fig-cap: \"STL Decomposition Results\"\n#| fig-cap-location: bottom\n#| warning: false\nfrom statsmodels.tsa.seasonal import STL\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n# Setting plot style and size\nsns.set_style(\"darkgrid\")\nplt.rc(\"figure\", figsize=(7, 5))\nplt.rc(\"font\", size=10)\n\n# Create a sample subset to plot\nsubset = transformed_data.loc[\"Germany\", \"KaggleRama\", \"Kaggle Advanced Techniques\"]\n\n# Plot the decomposed parts of the time series\nstl = STL(subset, period=365, seasonal=7, trend_deg=1)\ndecomp = stl.fit()\nfig = decomp.plot()\nfig.show();\n```\n\nAbove code decomposed the subset time series to three parts: trend, season, and residuals. Before creating ARIMA model, we still have to make sure that the residual part is non-stationary. We will test for stationarity with KPSS again and use differencing until it is stationary. This differencing degree will determine our I(d) parameter in the ARIMA model.\n\n```{python}\n#| warning: false\ndiff_test = kpss(decomp.resid)\nprint(f\"p-value of KPSS test is: {diff_test[1]}\")\n```\n\nResidual are stationary according to test so we won't need a differencing in our model and therefore d degree will be 0. For the AR(p) order, we will look at the autocorrelation graphs. For the MA(q) order, we will try values and select the model with the lowest **AIC (Akaike's Information Criteria)**.\n\n```{python}\n#| fig-align: center\n#| warning: false\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(decomp.resid);\nplot_pacf(decomp.resid);\n```\n\nPartial Autocorrelation Function shows statistically significant correlations at lags up to 7; thus we will start with AR(7). Now it is a good time to split our subset to train and test parts. Train data will be the first three years' data and the test will be last year's data.\n\n```{python}\n#| warning: false\n#| fig-align: center\nfrom statsmodels.tsa.forecasting.stl import STLForecast, STLForecastResults\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sktime.forecasting.model_selection import temporal_train_test_split\n\n# Split subset to train-test\ntrain_subset, test_subset_pretuning = temporal_train_test_split(subset, test_size=366)\n\n# Define STLForecast model\nstl_forecast = STLForecast(\n    train_subset,\n    model=ARIMA,\n    model_kwargs={\"order\": (7, 0, 0), \"trend\": \"t\"},\n    period=365,\n    trend=1095,\n    robust=False\n)\n\n# Fit the model\nmodel = stl_forecast.fit()\n\n# Forecast for the last year\nforecast = model.forecast(steps=366)\n\n# Create a function to plot both test data and forecast from the model to compare them visually.\ndef plot_forecasts(y_true, y_pred, title=\"Forecasts\", width=700, height=500):\n    # Create a DataFrame to store both series\n    series = pandas.DataFrame()\n    series[\"Observed\"] = y_true\n    series[\"Predicted\"] = y_pred\n    \n    # Define plot attributes\n    fig = px.line(\n        series, \n        width=width, \n        height=height, \n        title=title,\n        labels={\n            \"value\": \"Sales\",\n            \"variable\": \"\"\n        }\n    )\n    fig.show()\n\nplot_forecasts(test_subset_pretuning[\"num_sold\"], forecast, title=\"STL-ARIMA Forecasts with (7,0,0)\")\nmodel.summary()\n```\n\nThe forecast looks pretty good already; however, from the summary we detect that p-value for Ljung-Box test is smaller than 0.05. The null hypothesis of Ljung-Box test is that residuals of the model after fitting ARIMA does not contain autocorrelations. Smaller than 0.05 p-value shows that our model could not capture the autocorrelations of the model fully. So will try different q orders and select the one which has the smallest AIC value and the case where the residuals are significantly autocorrelated (p > 0.05).\n\n```{python}\n#| warning: false\n#| cache: true\nq_orders = np.arange(0, 8)\nresults = []\nfor q in q_orders:\n    stl_forecast = STLForecast(\n        train_subset,\n        model=ARIMA,\n        model_kwargs={\"order\": (7, 0, q), \"trend\": \"t\"},\n        period=365,\n        trend=1095,\n        robust=False,\n    )\n\n    model = stl_forecast.fit()\n    forecast = model.forecast(steps=366)\n    results.append(model.model_result.aic)\n\nprint(f\"Best q is {results.index(min(results))} with AIC value of {min(results)}\")\n```\n\nLet's check the summary again with q value of 7.\n\n```{python}\n#| warning: false\n#| fig-align: center\nstl_forecast = STLForecast(\n    train_subset,\n    model=ARIMA,\n    model_kwargs={\"order\": (7, 0, 7),\n                    \"trend\": \"t\"},\n    period=365,\n    trend=1095,\n    robust=False\n)\n\nmodel = stl_forecast.fit()\n\n# Forecast for the last year\nforecast = model.forecast(steps=366)\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(test_subset_pretuning[\"num_sold\"], forecast, title=\"STL-ARIMA Forecasts with (7,0,7)\")\n\nmodel.summary()\n```\n\nFinally calculate the mean absolute percentage error (MAPE) and R2 Score as a benchmark.\n\n```{python}\n#| code-fold: true\nfrom sklearn.metrics import mean_absolute_percentage_error, r2_score\n\nprint(f\"MAPE for STL-ARIMA model is: {mean_absolute_percentage_error(test_subset_pretuning, forecast)}\")\nprint(f\"R2 Score for STL-ARIMA model is: {r2_score(test_subset_pretuning, forecast)}\")\n```\n\n## Forecasting with Exogenous Regressors\n\nNow we will move to the forecasting which uses exogenous variables as features. To achieve that, firstly, we will convert the datetime of the series to tabular features and then add holidays. We will also use some libraries dedicated to feature extraction from time series.\n\n### ARIMAX\n\nFor ARIMAX model, we will create a `feature_matrix` which has the date related features and lagged values as predictors.\n\n```{python}\n#| warning: false\nimport featuretools as ft\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport holidays\nfrom statsmodels.tsa.deterministic import Fourier\n\n# First we will create an EntitySet\nentity_set = ft.EntitySet()\ndata = data.sort_values(by=[\"country\", \"store\", \"product\", \"date\"])\n\n# Specify index and time_index columns\nentity_set.add_dataframe(\n    dataframe_name=\"Datetime Features\",\n    dataframe=data,\n    index=\"row_id\",\n    time_index=\"date\",\n)\n\nfeature_matrix, feature_defs = ft.dfs(\n    entityset=entity_set,\n    target_dataframe_name=\"Datetime Features\",\n    trans_primitives=[\"DAY\", \"MONTH\", \"WEEKDAY\", \"YEAR\"],\n)\n\n# Label encode the year column\nyear_encoder = LabelEncoder()\nfeature_matrix[\"YEAR(date)\"] = year_encoder.fit_transform(feature_matrix[\"YEAR(date)\"])\n\n# Get the holidays for each country\nholidays_country = [holidays.country_holidays(i) for i in data[\"country\"].unique()]\n\n# Add the original date column for comparison\nfeature_matrix[\"date\"] = to_datetime(data[\"date\"])\n\n\n\"\"\"\nCheck dates for local holidays. \nIf there is a holiday on the date add their local name to holiday_name, else add \"No Holiday\".\n\"\"\"\nholiday_name = list()\nfor i in range(len(feature_matrix)):\n    if feature_matrix[\"country\"][i] == \"Belgium\":\n        if feature_matrix[\"date\"][i] in holidays_country[0]:\n            holiday_name.append(holidays_country[0].get(feature_matrix[\"date\"][i]))\n        else:\n            holiday_name.append(\"No Holiday\")\n    elif feature_matrix[\"country\"][i] == \"France\":\n        if feature_matrix[\"date\"][i] in holidays_country[1]:\n            holiday_name.append(holidays_country[1].get(feature_matrix[\"date\"][i]))\n        else:\n            holiday_name.append(\"No Holiday\")\n    elif feature_matrix[\"country\"][i] == \"Germany\":\n        if feature_matrix[\"date\"][i] in holidays_country[2]:\n            holiday_name.append(holidays_country[2].get(feature_matrix[\"date\"][i]))\n        else:\n            holiday_name.append(\"No Holiday\")\n    elif feature_matrix[\"country\"][i] == \"Italy\":\n        if feature_matrix[\"date\"][i] in holidays_country[3]:\n            holiday_name.append(holidays_country[3].get(feature_matrix[\"date\"][i]))\n        else:\n            holiday_name.append(\"No Holiday\")\n    elif feature_matrix[\"country\"][i] == \"Poland\":\n        if feature_matrix[\"date\"][i] in holidays_country[4]:\n            holiday_name.append(holidays_country[4].get(feature_matrix[\"date\"][i]))\n        else:\n            holiday_name.append(\"No Holiday\")\n    elif feature_matrix[\"country\"][i] == \"Spain\":\n        if feature_matrix[\"date\"][i] in holidays_country[5]:\n            holiday_name.append(holidays_country[5].get(feature_matrix[\"date\"][i]))\n        else:\n            holiday_name.append(\"No Holiday\")\n\nfeature_matrix[\"HolidayName\"] = holiday_name\n\n# Sort the dataframe so that date is sequential\nfeature_matrix = feature_matrix.sort_values(\n    by=[\"country\", \"store\", \"product\", \"date\"], ignore_index=True\n)\n\n\"\"\"\nCheck for the closeness of days to holidays. If a day is in the spectrum of 3 days before -\n3 days after of a holiday, label it as Close.\n\"\"\"\nholiday_closeness = list()\nfor i in range(len(feature_matrix)):\n    if feature_matrix[\"HolidayName\"][i] != \"No Holiday\":\n        holiday_closeness.append(\"Holiday\")\n    else:\n        if len(list(\"No Holiday\" == feature_matrix[\"HolidayName\"][i - 3 : i + 4])) == 0:\n            holiday_closeness.append(\"Close\")\n        elif False in list(\n            \"No Holiday\" == feature_matrix[\"HolidayName\"][i - 3 : i + 4]\n        ):\n            holiday_closeness.append(\"Close\")\n        else:\n            holiday_closeness.append(\"Distant\")\nfeature_matrix[\"HolidayCloseness\"] = holiday_closeness\n\n# One-hot encode holiday closeness\nfeature_matrix = pandas.get_dummies(\n    feature_matrix,\n    columns=[\n        \"country\",\n        \"store\",\n        \"product\",\n        \"DAY(date)\",\n        \"MONTH(date)\",\n        \"WEEKDAY(date)\",\n        \"YEAR(date)\",\n        \"HolidayCloseness\",\n    ],\n    drop_first=True,\n)\n\n# Deterministic Fourier Terms\n## Yearly\nyearly_fourier_terms = Fourier(365, 3)\nindex = np.arange(0, 1461)\nyearly_fourier_terms = yearly_fourier_terms.in_sample(index)\ncloning_func = lambda x: np.tile(x, 48)\nyearly_fourier_terms = yearly_fourier_terms.apply(func=cloning_func, axis=0)\nfeature_matrix = feature_matrix.join(yearly_fourier_terms, how=\"left\")\n\n# Edit columns\nfeature_matrix = pandas.DataFrame.join(\n    feature_matrix.drop(columns=[\"num_sold\", \"HolidayName\"]),\n    feature_matrix[\"num_sold\"],\n    how=\"left\",\n)\n\nfeature_matrix\n```\n\nThe feature matrix is almost ready for use in ARIMAX model. To summarize what we have done so far regarding the features, we have transformed the time points to categorical variables - day of the month, weekday, and year and added holidays. To better account for the effect of holidays, we have created three dummy holidays variables: Close, Distant and Holiday. Then to capture yearly seasonality we have added Fourier terms with period of 365. After creating the subset we can discard the country, product and store variables since we are training the models for each combination and they add nothing of value for forecasting in this case.\n\nLet's create the ARIMAX model and forecast with the same subset.\n\n```{python}\n#| warning: false\n#| fig-align: center\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Create the same subset but this time with features.\nsubset_reg = feature_matrix[\n    (feature_matrix[\"country_Germany\"] == 1)\n    & (feature_matrix[\"store_KaggleRama\"] == 1)\n    & (feature_matrix[\"product_Kaggle Recipe Book\"] == 1)\n]\nsubset_reg = subset_reg.drop(\n    columns=[\n        \"date\",\n        \"country_France\",\n        \"country_Germany\",\n        \"country_Poland\",\n        \"country_Italy\",\n        \"country_Spain\",\n        \"store_KaggleRama\",\n        \"product_Kaggle Getting Started\",\n        \"product_Kaggle Recipe Book\",\n        \"product_Kaggle for Kids: One Smart Goose\",\n    ]\n)\n\n# Train - Test Split\ntrain_reg, test_reg = temporal_train_test_split(subset_reg, test_size=366)\n\n# Define endogenous and exogenous variables\nendog = train_reg[\"num_sold\"]\nexog = train_reg.drop(columns=\"num_sold\")\n\n# Use the same orders as previous ARIMA model\narimax_model = ARIMA(\n    order=(3, 1, 3), endog=endog, trend=[0, 1], exog=exog, enforce_stationarity=True\n)\n\narimax_results = arimax_model.fit()\n\n# Forecast for the last year\nforecast_reg = arimax_results.forecast(366, exog=test_reg.drop(columns=\"num_sold\"))\n\ntest_reg = test_reg.set_index(forecast_reg.index)\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(test_reg[\"num_sold\"], forecast_reg, title=\"ARIMAX Forecasts\");\n```\n\nFrom the plot we can see that this model performs worse than STL-ARIMA model. Let's check ARIMAX model's MAPE score to quantify performance difference.\n\n```{python}\n#| code-fold: true\nprint(f\"MAPE for ARIMAX model is: {mean_absolute_percentage_error(test_reg['num_sold'], forecast_reg)}\")\nprint(f\"R2 Score for ARIMAX model is: {r2_score(test_reg['num_sold'], forecast_reg)}\")\n```\n\n### XGBoost\n\nLet's try the regression now with using XGBoost. We will again use the same feature matrix as exogenous variables but we'll also use previous values of `num_sold` as input variables.\n\n```{python}\n#| cache: true\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Create a function so that we can try with different window sizes\ndef prev_values_tabularizer(feature_matrix, endog, window_size, scaler=False):\n    # Create a list to store previous values\n    prev_values = list()\n    for i in range(len(feature_matrix)):\n        prev_values.append(feature_matrix[endog][i : i + window_size])\n\n    # Reshape from columns to rows\n    prev_values = [\n        pandas.Series(prev_values[i]).values.reshape([1, -1])\n        for i in range(len(prev_values))\n    ]\n\n    # Create placeholder DataFrame with zeros and change them with previous values\n    zeros_matrix = np.zeros(shape=[len(feature_matrix), window_size])\n    placeholder_df = pandas.DataFrame(zeros_matrix)\n    for i in range(len(placeholder_df)):\n        for j in range(window_size):\n            try:\n                placeholder_df[j][i] = prev_values[i][0][j]\n            except IndexError:\n                continue\n\n    # Add previous values to feature matrix, shift it by window size, remove first window size values from the feature matrix\n    feature_matrix = pandas.concat([feature_matrix, placeholder_df], axis=1)\n    feature_matrix[list(range(window_size))] = feature_matrix[\n        list(range(window_size))\n    ].shift(window_size)\n    feature_matrix = feature_matrix.drop(list(range(window_size)), axis=0)\n\n    # If scaler is passed, apply scaling to previous values of endogenous variable\n    if scaler != False:\n        feature_matrix[list(range(window_size))] = scaler.fit_transform(\n            feature_matrix[list(range(window_size))]\n        )\n\n    # Reorder the columns so that endogenous variables are at the far most right\n    reordered_columns = (\n        feature_matrix.columns.to_list()[: (-window_size - 1)]\n        + feature_matrix.columns.to_list()[(-window_size):]\n        + [feature_matrix.columns.to_list()[(-window_size - 1)]]\n    )\n    feature_matrix = feature_matrix[reordered_columns]\n\n    return feature_matrix\n\n\nscaler = MinMaxScaler()\n\n# We use 7 as window_size because of the previous partial autocorrelation graphs\nregression_matrix = prev_values_tabularizer(feature_matrix, \"num_sold\", 7)\n```\n\nWe have transformed the feature matrix suitable for forecasting with regression. Now we have to split it again to train and test subsets and create regression model with XGBoost.\n\n```{python}\n#| fig-align: center\nfrom xgboost import XGBRegressor\n\n# Create the same subset as previous examples and split\nsubset_xgboost = regression_matrix[\n    (regression_matrix[\"country_Germany\"] == 1)\n    & (regression_matrix[\"store_KaggleRama\"] == 1)\n    & (regression_matrix[\"product_Kaggle Recipe Book\"] == 1)\n]\nsubset_xgboost = subset_xgboost.drop(\n    columns=[\n        \"date\",\n        \"country_France\",\n        \"country_Germany\",\n        \"country_Poland\",\n        \"country_Italy\",\n        \"country_Spain\",\n        \"store_KaggleRama\",\n        \"product_Kaggle Getting Started\",\n        \"product_Kaggle Recipe Book\",\n        \"product_Kaggle for Kids: One Smart Goose\",\n    ]\n)\n\ntrain_subset_xgboost, test_subset_xgboost = temporal_train_test_split(\n    subset_xgboost, test_size=366\n)\n\n# Define train/test features and targets\ntrain_targets_xgboost = train_subset_xgboost[\"num_sold\"]\ntrain_features_xgboost = train_subset_xgboost.drop(columns=\"num_sold\")\ntest_targets_xgboost = test_subset_xgboost[\"num_sold\"]\ntest_features_xgboost = test_subset_xgboost.drop(columns=\"num_sold\")\n\n# Define model\nxgboost_model = XGBRegressor()\nxgboost_model.fit(train_features_xgboost, train_targets_xgboost)\n\n# Forecast for the last year\nforecast_xgboost = pandas.DataFrame(xgboost_model.predict(test_features_xgboost))\nforecast_xgboost.set_index(test_features_xgboost.index, inplace=True)\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(test_targets_xgboost, forecast_xgboost, title=\"XGBoost Forecasts\");\n```\n\nFrom the plot we can see that regression model with XGBoost performs better than ARIMAX model. Let's check the scores again.\n\n```{python}\n#| code-fold: true\nprint(f\"MAPE for XGBoost Model is: {mean_absolute_percentage_error(test_targets_xgboost, forecast_xgboost)}\")\nprint(f\"R2 Score for XGBoost Model is: {r2_score(test_targets_xgboost, forecast_xgboost)}\")\n```\n\nScores surely seems better than our other forecasting attempts with STL-ARIMA and ARIMAX, though it can be said that this regression model tends not to undershoot. It is also more successful to capture New Year's Eve sales than other models.\n\nIt is still early to call this the best model, since we didn't tuned thoroughly the models' hyperparamaters and there are deep learning models to try. Now we'll move to the business case definition and after that, hyperparameter tuning of STL-ARIMA, ARIMAX, and XGBoost models. For deep learning part of this case please refer to [Part 2](/posts/book-forecasting/book_sales_forecasting_deep_learning_part.html).\n\n# Business Case Objective Function\n\nTo compare different models, so far, we have used Mean Absolute Percentage Error and R2 Scores. We have also plotted our forecasts and compared them with test targets. Now we will approach the problem from a different perspective.\n\nInstead of minimizing error, we will try to minimize our cost. It didn't matter whether a sales forecast is for example 20 units more or less; but having 20 more books in the inventory and opportunity cost of selling 20 less books may not be the same. Our custom function will define exactly that.\n\nOur assumptions are:\n\n- Every book is sold for €20\n- Book dimensions: 210x148x10mm (A5)\n- Storage cost: €100 per month\n- Warehouse dimensions: 2x2x3m\n- Warehouse space utilization: %15\n\nWith these assumptions, one warehouse can store approximately 6000 books for a storage fee of €100 per month. The scenario is that for every each book which the forecast undershoots the true sales, we have the opportunity cost of one book. If the forecast overshoots the true sales, then we can store the book in our warehouse by renting it. After renting the warehouse, each subsequent undershoot can be compensated from our inventory - effectively eliminating the opportunity cost but if storage limit is exceeded then we have to rent another warehouse increasing our monthly cost to €200.\n\nWe will use the function `cost_function()` to calculate this metric.\n\n```{python}\n# Define cost function for our case\ndef cost_function(storage, y_true, y_pred, book_price, storage_monthly_rent):\n  if storage == 0:\n    if y_pred <= y_true:\n      cost = book_price*(y_true - y_pred)\n      to_storage = 0\n      return cost, to_storage\n    elif y_pred > y_true:\n      cost = storage_monthly_rent/30\n      to_storage = y_pred - y_true\n      return cost, to_storage\n  elif storage > 0:\n    if y_pred <= y_true:\n      if (y_true - y_pred) < storage:\n        cost = storage_monthly_rent/30\n        to_storage = y_pred - y_true\n        return cost, to_storage\n      elif (y_true - y_pred) > storage:\n        cost = book_price*(y_true - y_pred - storage) + storage_monthly_rent/30\n        to_storage = -storage\n        return cost, to_storage\n    elif y_pred > y_true:\n      cost = storage_monthly_rent/30\n      to_storage = y_pred - y_true\n      return cost, to_storage\n```\n\n\n# Hyperparameter Tuning with Optuna\n\nNow we will tune and select hyperparameters for our models with the lowest cost. To reduce bias and get better generalization from the models, we will split the data to train - test - validation. While searching for the model with lowest cost using test data it may overfit. Therefore, we will tune the parameters using test dataset and after that check the performance using validation data.\n\nAlso earlier we were using test features for forecasts but feature matrix contains future `num_sold` values in it; thus, it wasn't a legitimate forecast. In this part we will also change the way forecasts are created so that the model uses its own forecasts as past values instead of real sales.\n\n## STL-ARIMA Tuning\n\n```{python}\n#| echo: false\nsubset = pandas.read_csv(\"subset_sklearn.csv\")\nsubset = subset.drop(\n  columns=[\n    \"Unnamed: 0\",\n    \"0\",\n    \"1\",\n    \"2\",\n    \"3\",\n    \"4\",\n    \"5\",\n    \"6\"]\n)\n```\n\n\n```{python}\n#| cache: true\n#| warning: false\nimport optuna\nimport statsmodels\n\n# Create validation data for STL-Arima\ntrain_subset = subset[\"num_sold\"][0:730].reset_index().drop(columns=\"index\")\ntest_subset = subset[\"num_sold\"][730:1095].reset_index().drop(columns=\"index\")\nval_subset = subset[\"num_sold\"][1095:1461].reset_index().drop(columns=\"index\")\n\n# Define Optuna Objective\ndef stl_objective(trial):\n\n    # Select ranges for p, d, q orders\n    p_order = trial.suggest_int(\"p_order\", 0, 10, log=False)\n    d_order = trial.suggest_int(\"d_order\", 0, 2, log=False)\n    q_order = trial.suggest_int(\"q_order\", 0, 10, log=False)\n    robust_bool = trial.suggest_categorical(\"robust_mode\", [False, True])\n    trend_deg = trial.suggest_int(\"trend_deg\", 0, 1)\n    seasonal_deg = trial.suggest_int(\"seasonal_deg\", 0, 1)\n    low_pass_deg = trial.suggest_int(\"low_pass_deg\", 0, 1)\n    seasonal_jump = trial.suggest_int(\"seasonal_jump\", 1, 3)\n    trend_jump = trial.suggest_int(\"trend_jump\", 1, 3)\n    low_pass_jump = trial.suggest_int(\"low_pass_jump\", 1, 3)\n\n    # Define model with trial variables\n    stl_forecast_tuned = STLForecast(\n        train_subset,\n        model=statsmodels.tsa.arima.model.ARIMA,\n        model_kwargs={\"order\": (p_order, d_order, q_order), \"trend\": \"n\"},\n        period=365,\n        trend=731,\n        trend_deg=trend_deg,\n        seasonal_deg=seasonal_deg,\n        low_pass_deg=low_pass_deg,\n        seasonal_jump=seasonal_jump,\n        trend_jump=trend_jump,\n        low_pass_jump=low_pass_jump,\n        robust=robust_bool,\n    )\n\n    # Fit and forecast test data\n    model = stl_forecast_tuned.fit()\n    forecast = model.forecast(steps=365)\n\n    # Reset index of forecasts\n    forecast = forecast.reset_index().drop(columns=\"index\")\n\n    # Create a loop to calculate cumulative cost of the forecast\n    storage = 0\n    cumulative_cost = 0\n    book_price = 20\n    monthly_storage_cost = 100\n    for step in range(len(test_subset)):\n\n        # Get the cost and difference for storage for the current step\n        cost, to_storage = cost_function(\n            storage, test_subset[\"num_sold\"][step], forecast[0][step], book_price, monthly_storage_cost\n        )\n\n        # Add cost to cumulative cost and storage difference to storage\n        cumulative_cost += int(cost)\n        storage += int(to_storage)\n\n    total_cost = cumulative_cost + storage * book_price\n    return total_cost\n\n\n# Create Optuna Study and Minimize total_cost\nstl_study = optuna.create_study(direction=\"minimize\")\nstl_study.optimize(stl_objective, n_trials=20)\n```\n\nNow we'll create the model with best parameters to get tuned forecasts.\n\n```{python}\n#| warning: false\n# Get the parameters of the best model\nbest_stl_model_parameters = stl_study.best_params\n\n# Define STL-Arima model with the best parameters and train on test data\nstl_forecast = STLForecast(\n    test_subset,\n    model=statsmodels.tsa.arima.model.ARIMA,\n    model_kwargs={\n        \"order\": (\n            best_stl_model_parameters[\"p_order\"],\n            best_stl_model_parameters[\"d_order\"],\n            best_stl_model_parameters[\"q_order\"],\n        ),\n        \"trend\": \"n\",\n    },\n    period=365,\n    trend=1095,\n    robust=best_stl_model_parameters[\"robust_mode\"],\n    trend_deg=best_stl_model_parameters[\"trend_deg\"],\n    seasonal_deg=best_stl_model_parameters[\"seasonal_deg\"],\n    low_pass_deg=best_stl_model_parameters[\"low_pass_deg\"],\n    seasonal_jump=best_stl_model_parameters[\"seasonal_jump\"],\n    trend_jump=best_stl_model_parameters[\"trend_jump\"],\n    low_pass_jump=best_stl_model_parameters[\"low_pass_jump\"],\n)\n\n# Fit and forecast test data\ntuned_stl_model = stl_forecast.fit()\nforecast_stl_arima_tuned = tuned_stl_model.forecast(steps=366)\nforecast_stl_arima_tuned = forecast_stl_arima_tuned.reset_index().drop(columns=\"index\")\n\n# Create a loop to calculate cumulative cost of the forecast\nstorage = 0\ncumulative_cost = 0\nbook_price = 20\nmonthly_storage_cost = 100\nfor step in range(len(val_subset)):\n\n    # Get the cost and difference for storage for the current step\n    cost, to_storage = cost_function(\n        storage,\n        val_subset[\"num_sold\"][step],\n        forecast_stl_arima_tuned[0][step],\n        book_price,\n        monthly_storage_cost,\n    )\n\n    # Add cost to cumulative cost and storage difference to storage\n    cumulative_cost += int(cost)\n    storage += int(to_storage)\n\nstl_arima_total_cost = cumulative_cost + storage * book_price\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(val_subset, forecast_stl_arima_tuned, title=\"STL ARIMA Tuned Forecasts\");\n```\n\nCalculate the MAPE and R2 Scores and the total cost according to our metric.\n\n```{python}\n#| code-fold: true\nprint(f\"MAPE for Earlier STL-ARIMA Model is: {mean_absolute_percentage_error(test_subset_pretuning, forecast)}\")\nprint(f\"MAPE for Tuned STL-ARIMA is: {mean_absolute_percentage_error(val_subset['num_sold'], forecast_stl_arima_tuned[0])}\\n\")\nprint(f\"R2 Score for Earlier STL-ARIMA Model is: {r2_score(test_subset_pretuning, forecast)}\")\nprint(f\"R2 Score for Tuned STL-ARIMA is: {r2_score(val_subset['num_sold'], forecast_stl_arima_tuned[0])}\\n\")\n\nprint(f\"Total Cost for tuned STL-ARIMA is: €{stl_arima_total_cost:,.2f}\")\n```\n\n## ARIMAX Tuning\n\n```{python}\n#| code-fold: true\n#| code-summary: \"Optuna ARIMAX Tuning Code\"\n#| cache: true\n#| warning: false\n# Create validation data for ARIMAX\ntrain_subset = subset[0:730]\ntest_subset = subset[730:1095]\nval_subset = subset[1095:1461]\n\n# Define endogenous and exogenous variables\ntrain_targets = pandas.DataFrame(train_subset[\"num_sold\"]).reset_index().drop(columns=\"index\")\ntrain_features = pandas.DataFrame(train_subset.drop(columns=\"num_sold\")).reset_index().drop(columns=\"index\")\n\ntest_targets = pandas.DataFrame(test_subset[\"num_sold\"]).reset_index().drop(columns=\"index\")\ntest_features = pandas.DataFrame(test_subset.drop(columns=\"num_sold\")).reset_index().drop(columns=\"index\")\n\nval_targets = pandas.DataFrame(val_subset[\"num_sold\"]).reset_index().drop(columns=\"index\")\nval_features = pandas.DataFrame(val_subset.drop(columns=\"num_sold\")).reset_index().drop(columns=\"index\")\n\n# Define Optuna Objective\ndef arimax_objective(trial):\n\n  # Select ranges for p, d, q orders\n  p_order = trial.suggest_int(\"p_order\", 0, 10, log=False)\n  d_order = trial.suggest_int(\"d_order\", 0, 2, log=False)\n  q_order = trial.suggest_int(\"q_order\", 0, 10, log=False)\n\n  # Trend order according to d_order differencing\n  trend_order = list(np.zeros(d_order))\n  trend_order.append(1)\n\n  # Define model with trial variables\n  arimax_model = statsmodels.tsa.arima.model.ARIMA(\n    order=(p_order, d_order, q_order),\n    endog=train_targets,\n    trend=trend_order,\n    exog=train_features\n  )\n  \n  # Fit the model\n  arimax_model_results = arimax_model.fit()\n\n  # Forecast for the last year\n  forecast_arimax_tuned = arimax_model_results.forecast(365, exog=test_features)\n  forecast_arimax_tuned = pandas.DataFrame(forecast_arimax_tuned).reset_index().drop(columns=\"index\")\n\n  # Create a loop to calculate cumulative cost of the forecast\n  storage = 0\n  cumulative_cost = 0\n  book_price = 20\n  monthly_storage_cost = 100\n  for step in range(len(test_targets)):\n\n    # Get the cost and difference for storage for the current step\n    cost, to_storage = cost_function(\n      storage, \n      test_targets[\"num_sold\"][step], \n      forecast_arimax_tuned[\"predicted_mean\"][step], \n      book_price, \n      monthly_storage_cost\n      )\n\n    # Add cost to cumulative cost and storage difference to storage\n    cumulative_cost += int(cost)\n    storage += int(to_storage)\n\n  total_cost = cumulative_cost + storage*book_price\n  return total_cost\n\n# Create Optuna Study and Minimize total_cost\narimax_study = optuna.create_study(direction=\"minimize\")\narimax_study.optimize(arimax_objective, n_trials=20)\n```\n\n```{python}\n#| code-fold: true\n#| code-summary: \"Forecasts with Best ARIMAX Parameters Code\"\n#| cache: true\n#| warning: false\n# Get the parameters of the best model\nbest_arimax_model_parameters = arimax_study.best_params\n\n# Trend order according to d_order differencing\ntrend_order = list(np.zeros(best_arimax_model_parameters[\"d_order\"]))\ntrend_order.append(1)\n\n# Define Arimax model with the best parameters and train on test data\narimax_model = statsmodels.tsa.arima.model.ARIMA(\n  order=(best_arimax_model_parameters[\"p_order\"],\n  best_arimax_model_parameters[\"d_order\"],\n  best_arimax_model_parameters[\"q_order\"]),\n  endog=test_targets,\n  trend=[0],\n  exog=test_features)\n\n# Fit the model\narimax_model_results = arimax_model.fit()\n\n# Forecast for the last year\nforecast_arimax_tuned = arimax_model_results.forecast(366, exog=val_features)\nforecast_arimax_tuned = pandas.DataFrame(forecast_arimax_tuned).reset_index().drop(columns=\"index\")\n\n# Create a loop to calculate cumulative cost of the forecast\nstorage = 0\ncumulative_cost = 0\nbook_price = 20\nmonthly_storage_cost = 100\nfor step in range(len(val_targets)):\n\n  # Get the cost and difference for storage for the current step\n  cost, to_storage = cost_function(\n    storage, \n    val_targets[\"num_sold\"][step], \n    forecast_arimax_tuned[\"predicted_mean\"][step], \n    book_price, \n    monthly_storage_cost)\n\n  # Add cost to cumulative cost and storage difference to storage\n  cumulative_cost += int(cost)\n  storage += int(to_storage)\n\narimax_total_cost = cumulative_cost + storage*book_price\n\n# Plot both test data and forecast from the model to compare them visually.\nplot_forecasts(val_targets, forecast_arimax_tuned, title=\"ARIMAX Tuned Forecasts\");\n```\n\n```{python}\n#| code-fold: true\nprint(f\"MAPE for Earlier ARIMAX model is: {mean_absolute_percentage_error(test_reg['num_sold'], forecast_reg)}\")\nprint(f\"MAPE for Tuned ARIMAX model is: {mean_absolute_percentage_error(val_targets['num_sold'], forecast_arimax_tuned['predicted_mean'])}\\n\")\n\nprint(f\"R2 Score for Earlier ARIMAX model is: {r2_score(test_reg['num_sold'], forecast_reg)}\")\nprint(f\"R2 Score for Tuned ARIMAX model is: {r2_score(val_targets['num_sold'], forecast_arimax_tuned['predicted_mean'])}\\n\")\n\nprint(f\"Total Cost for tuned ARIMAX model is: €{arimax_total_cost:,.2f}\")\n```\n\n## XGBoost Tuning\n\n```{python}\n#| code-fold: true\n#| code-summary: \"XGBoost Optuna Tuning and regression_forecast Function\"\n#| cache: true\n#| warning: false\nfrom xgboost import XGBRegressor\n\n# Create train - test- validation data\nsubset_xgboost_tuned = regression_matrix[\n    (regression_matrix[\"country_Germany\"] == 1)\n    & (regression_matrix[\"store_KaggleRama\"] == 1)\n    & (regression_matrix[\"product_Kaggle Recipe Book\"] == 1)\n]\nsubset_xgboost_tuned = subset_xgboost_tuned.drop(\n    columns=[\n        \"date\",\n        \"country_France\",\n        \"country_Germany\",\n        \"country_Poland\",\n        \"country_Italy\",\n        \"country_Spain\",\n        \"store_KaggleRama\",\n        \"product_Kaggle Getting Started\",\n        \"product_Kaggle Recipe Book\",\n        \"product_Kaggle for Kids: One Smart Goose\",\n    ]\n)\n\n# Define train - test - validation features and targets\ntrain_subset_xgboost_tuned, test_subset_xgboost_tuned = temporal_train_test_split(\n    subset_xgboost_tuned, train_size=730\n)\ntest_subset_xgboost_tuned, val_subset_xgboost_tuned = temporal_train_test_split(\n    test_subset_xgboost_tuned, test_size=366\n)\n\ntrain_targets_xgboost_tuned = (\n    pandas.DataFrame(train_subset_xgboost_tuned[\"num_sold\"])\n    .reset_index()\n    .drop(columns=\"index\")\n)\ntrain_features_xgboost_tuned = (\n    pandas.DataFrame(train_subset_xgboost_tuned.drop(columns=\"num_sold\"))\n    .reset_index()\n    .drop(columns=\"index\")\n)\n\ntest_targets_xgboost_tuned = (\n    pandas.DataFrame(test_subset_xgboost_tuned[\"num_sold\"])\n    .reset_index()\n    .drop(columns=\"index\")\n)\ntest_features_xgboost_tuned = (\n    pandas.DataFrame(test_subset_xgboost_tuned.drop(columns=\"num_sold\"))\n    .reset_index()\n    .drop(columns=\"index\")\n)\n\nval_targets_xgboost_tuned = (\n    pandas.DataFrame(val_subset_xgboost_tuned[\"num_sold\"])\n    .reset_index()\n    .drop(columns=\"index\")\n)\nval_features_xgboost_tuned = (\n    pandas.DataFrame(val_subset_xgboost_tuned.drop(columns=\"num_sold\"))\n    .reset_index()\n    .drop(columns=\"index\")\n)\n\n# Define forecasting function\ndef regression_forecast(\n    train_features, forecast_features, model, window_size, scaled=False\n):\n\n    # Add the last column of training data\n    history = train_features.tail(1)\n    forecasts = list()\n    for _ in range(len(forecast_features)):\n        prediction = model.predict(history.tail(1))\n\n        # Get features of data to be forecasted\n        next_sample = forecast_features.head(1)\n\n        # Shift previous values to 1 left\n        next_sample[list(range(window_size))] = (\n            history.tail(1)[list(range(window_size))].shift(-1, axis=1).values\n        )\n\n        # If scaled, inverse transform\n        if scaled != False:\n            next_sample[list(range(window_size))] = scaler.inverse_transform(\n                next_sample[list(range(window_size))]\n            )\n\n            # Add prediction to the right\n            next_sample[window_size - 1] = prediction\n\n            # Scale values again before forecasting\n            next_sample[list(range(window_size))] = scaler.transform(\n                next_sample[list(range(window_size))]\n            )\n        else:\n            # Add prediction to the right\n            next_sample[window_size - 1] = prediction\n\n        # Add last values to history\n        history = pandas.concat([history, next_sample], ignore_index=True, axis=0)\n\n        # Drop uppermost row from forecast features\n        forecast_features = forecast_features.drop(index=min(forecast_features.index))\n\n        # Add current prediction the forecasts as a seperate list\n        forecasts.append(prediction)\n\n    return pandas.DataFrame(forecasts, columns=[\"prediction\"])\n\n\n# Define Optuna Objective\ndef xgboost_objective(trial):\n\n    # Define trial variables\n    booster = trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"])\n\n    if booster in [\"gbtree\", \"dart\"]:\n        eta = trial.suggest_float(\"eta\", 0, 1)\n        gamma = trial.suggest_int(\"gamma\", 0, 2)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 50)\n        min_child_weight = trial.suggest_int(\"min_child_weight\", 0, 20)\n        subsample = trial.suggest_float(\"subsample\", 0.5, 1)\n        reg_lambda = trial.suggest_float(\"lambda\", 0, 5)\n        alpha = trial.suggest_float(\"alpha\", 0, 5)\n\n        # Define model with trial variables\n        xgboost_model = XGBRegressor(\n            booster=booster,\n            learning_rate=eta,\n            min_split_loss=gamma,\n            max_depth=max_depth,\n            min_child_weight=min_child_weight,\n            subsample=subsample,\n            reg_lambda=reg_lambda,\n            reg_alpha=alpha,\n        )\n    else:\n        reg_lambda = trial.suggest_float(\"lambda\", 0, 5)\n        alpha = trial.suggest_float(\"alpha\", 0, 5)\n        updater = trial.suggest_categorical(\"updater\", [\"shotgun\", \"coord_descent\"])\n        feature_selector = trial.suggest_categorical(\n            \"feature_selector\", [\"cyclic\", \"shuffle\"]\n        )\n\n        # Define model with trial variables\n        xgboost_model = XGBRegressor(\n            booster=booster,\n            reg_lambda=reg_lambda,\n            reg_alpha=alpha,\n            updater=updater,\n            feature_selector=feature_selector,\n        )\n\n    # Fit the model\n    xgboost_model.fit(train_features_xgboost_tuned, train_targets_xgboost_tuned)\n\n    # Forecast for the test data but this time using model's predicted values as past values\n    forecast_xgboost_tuned = regression_forecast(\n        train_features_xgboost_tuned, \n        test_features_xgboost_tuned, \n        xgboost_model, \n        7\n    )\n\n    # Create a loop to calculate cumulative cost of the forecast\n    storage = 0\n    cumulative_cost = 0\n    book_price = 20\n    monthly_storage_cost = 100\n    for step in range(len(test_targets_xgboost_tuned)):\n\n        # Get the cost and difference for storage for the current step\n        cost, to_storage = cost_function(\n            storage,\n            test_targets_xgboost_tuned[\"num_sold\"][step],\n            forecast_xgboost_tuned[\"prediction\"][step],\n            book_price,\n            monthly_storage_cost,\n        )\n\n        # Add cost to cumulative cost and storage difference to storage\n        cumulative_cost += int(cost)\n        storage += int(to_storage)\n\n    total_cost = cumulative_cost + storage * book_price\n    return total_cost\n\n\n# Create Optuna Study and Minimize total_cost\nxgboost_study = optuna.create_study(direction=\"minimize\")\nxgboost_study.optimize(xgboost_objective, n_trials=20)\n```\n\n```{python}\n#| code-fold: true\n#| code-summary: \"XGBoost Model with Best Parameters Code\"\nbest_xgboost_model_parameters = xgboost_study.best_params\n\nif best_xgboost_model_parameters[\"booster\"] in [\"gbtree\", \"dart\"]:\n    # Define model with trial variables\n    xgboost_model = XGBRegressor(\n        booster=best_xgboost_model_parameters[\"booster\"],\n        learning_rate=best_xgboost_model_parameters[\"eta\"],\n        min_split_loss=best_xgboost_model_parameters[\"gamma\"],\n        max_depth=best_xgboost_model_parameters[\"max_depth\"],\n        min_child_weight=best_xgboost_model_parameters[\"min_child_weight\"],\n        subsample=best_xgboost_model_parameters[\"subsample\"],\n        reg_lambda=best_xgboost_model_parameters[\"lambda\"],\n        reg_alpha=best_xgboost_model_parameters[\"alpha\"]\n    )\nelse:\n    # Define model with trial variables\n    xgboost_model = XGBRegressor(\n        booster=best_xgboost_model_parameters[\"booster\"],\n        reg_lambda=best_xgboost_model_parameters[\"lambda\"],\n        reg_alpha=best_xgboost_model_parameters[\"alpha\"],\n        updater=best_xgboost_model_parameters[\"updater\"],\n        feature_selector=best_xgboost_model_parameters[\"feature_selector\"]\n    )\n\n# Fit the model with the best parameters\nxgboost_model.fit(test_features_xgboost_tuned, test_targets_xgboost_tuned)\n\n# Forecast for the test data but this time using model's predicted values as past values\nforecast_xgboost_tuned = regression_forecast(\n  test_features_xgboost_tuned,\n  val_features_xgboost_tuned,\n  xgboost_model,\n  7)\n\n# Create a loop to calculate cumulative cost of the forecast\nstorage = 0\ncumulative_cost = 0\nbook_price = 20\nmonthly_storage_cost = 100\nfor step in range(len(val_targets_xgboost_tuned)):\n\n    # Get the cost and difference for storage for the current step\n    cost, to_storage = cost_function(\n      storage, \n      val_targets_xgboost_tuned[\"num_sold\"][step], \n      forecast_xgboost_tuned[\"prediction\"][step], \n      book_price, \n      monthly_storage_cost)\n\n    # Add cost to cumulative cost and storage difference to storage\n    cumulative_cost += int(cost)\n    storage += int(to_storage)\n\nxgboost_total_cost = cumulative_cost + storage*book_price\n\n# Plot the forecasts compared to validation data\nplot_forecasts(val_targets_xgboost_tuned, forecast_xgboost_tuned, title=\"XGBoost Tuned Forecasts\");\n```\n\n```{python}\n#| code-fold: true\nprint(f\"MAPE for Earlier XGBoost model is: {mean_absolute_percentage_error(test_targets_xgboost, forecast_xgboost)}\")\nprint(f\"MAPE for Tuned XGBoost model is: {mean_absolute_percentage_error(val_targets_xgboost_tuned, forecast_xgboost_tuned)}\\n\")\n\nprint(f\"R2 Score for Earlier XGBoost model is: {r2_score(test_targets_xgboost, forecast_xgboost)}\")\nprint(f\"R2 Score for Tuned XGBoost model is: {r2_score(val_targets_xgboost_tuned, forecast_xgboost_tuned)}\\n\")\n\nprint(f\"Total Cost for Tuned XGBoost model is: €{xgboost_total_cost:,.2f}\")\n```\n\nTuning for XGBoost model loses some accuracy due to the difference between lagged value features. Earlier model was using real targets as lagged value, while this tuned and correct model uses its own forecasts as lagged values. We'll create and tune deep learning models on\n\n<p align=\"center\">\n<p style=\"font-size:25px\">\n    <a href=\"/posts/book-forecasting/book_sales_forecasting_deep_learning_part.html\"><b>Part 2 -></b></a>\n</p>\n</p>"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"highlight-style":"breezedark","toc":true,"number-sections":true,"output-file":"book_sales_forecasting.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","theme":"flatly","title-block-banner":true,"title":"Book Sales Forecasting: Comparison of Different Models Part 1","description":"From Classical Statistical Models to XGBoost","date":"2022-12-19","image":"images/forecast.png","categories":["python","time series forecasting","xgboost","arima","optuna","hyperparameter tuning"],"jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}}}